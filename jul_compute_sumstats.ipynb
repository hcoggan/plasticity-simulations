{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "This is a record of all the code used to compute summary stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from numba import njit, prange\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from sklearn.multiclass import OneVsRestClassifier as classifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelBinarizer\n",
    "from sklearn.metrics import log_loss, roc_curve\n",
    "import optuna\n",
    "from sklearn.cluster import AgglomerativeClustering as clustering\n",
    "\n",
    "base_filepath = \"/Users/44749/Documents/PhD_work/datasets_for_analysis/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#choose samples, making sure they're not too close together\n",
    "@njit()\n",
    "def choose_samples(sample_locs, no_samples):\n",
    "    chosen_indices, chosen_locs = [0], [sample_locs[0]] #take the first sample always, assume a point chosen at random\n",
    "    #of the available samples, take the one that maximises the total distance from all current samples\n",
    "    while len(chosen_indices) < no_samples:\n",
    "        furthest_current_index, max_total_distance = 0, 0\n",
    "        for num, loc in enumerate(sample_locs):\n",
    "            if num not in chosen_indices:\n",
    "                total_distance = sum([np.sqrt((loc[0]-x)**2 + (loc[1]-y)**2 + (loc[2]-z)**2) for [x, y, z] in chosen_locs])\n",
    "                if total_distance > max_total_distance:\n",
    "                    max_total_distance = total_distance\n",
    "                    furthest_current_index = num\n",
    "        chosen_indices.append(furthest_current_index)\n",
    "        chosen_locs.append(sample_locs[furthest_current_index])\n",
    "    return chosen_indices\n",
    "\n",
    "#calculate the number of mutations fixed in n samples\n",
    "@njit()\n",
    "def count_fixation_number(mut_matrix, num_samples):\n",
    "    number_fixed = 0\n",
    "    for row in mut_matrix:\n",
    "        num_samples_fixed = 0\n",
    "        for read in row:\n",
    "            if read == 1.0:\n",
    "                num_samples_fixed += 1\n",
    "        if num_samples_fixed == num_samples:\n",
    "            number_fixed += 1\n",
    "    return number_fixed\n",
    "\n",
    "\n",
    "prim_read_threshold = 10/160\n",
    "second_read_threshold = 4/160\n",
    "\n",
    "#calculate output summary statistics for a given number of samples, excluding clonal mutations\n",
    "def calculate_output_stats(filename, prefix, base_filepath, number, no_samples):\n",
    "    #load files\n",
    "    mutdict = np.load(base_filepath+filename+\"/\"+prefix+\"_\"+str(number)+\"_mutdict.npy\", allow_pickle=True)\n",
    "    sample_locs = np.load(base_filepath+filename+\"/\"+prefix+\"_\"+str(number)+\"_sample_locs.npy\", allow_pickle=True)\n",
    "    #to mimic actual sampling procedure\n",
    "    chosen_indices = choose_samples(sample_locs, no_samples)\n",
    "    sample_dicts = [mutdict[index] for index in chosen_indices]\n",
    "    #look at all mutations seen\n",
    "    all_muts = []\n",
    "    for dict in sample_dicts:\n",
    "        all_muts += list(dict.keys()) #look at all mutations seen in this sample\n",
    "    all_unique_mutations = np.unique(all_muts) #all mutations in all samples\n",
    "    mut_matrix = np.zeros((len(all_unique_mutations), no_samples))\n",
    "    for m, mut in enumerate(all_unique_mutations):\n",
    "        for n, dict in enumerate(sample_dicts):\n",
    "            if mut in dict:\n",
    "                mut_matrix[m][n] = dict[mut] #zero by default\n",
    "    min_ccfs = np.min(mut_matrix, axis=1)\n",
    "    non_clonal_muts = np.where(min_ccfs < 1.0)[0] #not clonal everywhere\n",
    "    mut_matrix = mut_matrix[non_clonal_muts] #filter out all of the clonal ones\n",
    "    #we need it to be detected above 10 reads somewhere; 4 reads in the relevant sample is already built in\n",
    "    max_ccfs = np.max(mut_matrix, axis=1)\n",
    "    sufficient = np.where(max_ccfs >= prim_read_threshold)[0]\n",
    "    mut_matrix = mut_matrix[sufficient]\n",
    "    #now to actually calculate summary statistics!\n",
    "    #0. all mutations detected subclonally (i.e. with CCF < 1.0 in at least one sample)\n",
    "    no_muts = len(mut_matrix)\n",
    "    #1. all mutations detected in 1 sample only\n",
    "    appearances = np.count_nonzero(mut_matrix, axis=1)\n",
    "    private = np.where(appearances == 1)[0]\n",
    "    num_private = len(private)\n",
    "    #2-no_samples, ... number fixed in 1, ... no_samples-1 samples\n",
    "    fixed_muts = []\n",
    "    for s in range(1, no_samples):\n",
    "        fixed_muts.append(count_fixation_number(mut_matrix, s)) #count number fixed in s samples- here it MAY THEN APPEAR IN A NUMBER OF OTHER SAMPLES, JUST NOT FIXED\n",
    "    #no_samples+1: number of mutations fixed AND private\n",
    "    private_ccfs = np.max(mut_matrix[private], axis=1) #the maximum CCF of a private mutation will be its CCF in that sample\n",
    "    fixed_and_private = np.where(private_ccfs == 1.0)[0]\n",
    "    num_fixed_private = len(fixed_and_private)\n",
    "    #no_samples + 2-2*no_samples-1- the number of mutations appearing in 2, ... no_samples samples\n",
    "    num_appearing = []\n",
    "    for s in range(2, no_samples+1):\n",
    "        num_appearing.append(len(np.where(appearances == s)[0]))\n",
    "    return [no_muts, num_private] + fixed_muts + [num_fixed_private] + num_appearing    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "For full run of tumours at mutation rate 0.6 per cell division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"27Jul_gen_0.6\"]*3\n",
    "prefixes = [\"nosel\", \"lowsel\", \"highsel\"]\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 5000\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"27Jul_1.0_0.6\"]*2\n",
    "prefixes = [\"lowsel\", \"highsel\"]\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "max_counts = 5000\n",
    "\n",
    "\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"27Jul_0.98_0.6\"]*2\n",
    "prefixes = [\"lowsel\", \"highsel\"]\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 5000\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"27Jul_0.95_0.6\"]*1\n",
    "prefixes = ['highsel'] #[\"lowsel\", \"highsel\"]\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 5000\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"27Jul_0.9_0.6\"]*1\n",
    "prefixes = [\"lowsel\"] #\"highsel\"]\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 5000\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"27Jul_0.75_0.6\"]*2\n",
    "prefixes = [\"lowsel\", \"highsel\"]\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 5000\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"27Jul_0.5_0.6\"]*2\n",
    "prefixes = [\"lowsel\", \"highsel\"]\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 5000\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"27Jul_0.0_0.6\"]*2\n",
    "prefixes = [\"lowsel\", \"highsel\"]\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 5000\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"27Jul_1.0_0.4\"]*2\n",
    "prefixes = [\"lowsel\", \"highsel\"]\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 5000\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"27Jul_gen_0.4\"]*3\n",
    "prefixes = [\"nosel\", \"lowsel\", \"highsel\"]\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 5000\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"27Jul_0.98_0.4\"]*1\n",
    "prefixes = [\"lowsel\"]# \"highsel\"]\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 5000\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"27Jul_0.95_0.4\"]*1\n",
    "prefixes = [\"lowsel\"]# \"highsel\"]\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 5000\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"27Jul_0.9_0.4\"]*1\n",
    "prefixes = [\"lowsel\"]# \"highsel\"]\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 5000\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"27Jul_0.75_0.4\"]*2\n",
    "prefixes = [\"lowsel\", \"highsel\"]\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 5000\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"27Jul_0.5_0.4\"]*2\n",
    "prefixes = [\"lowsel\", \"highsel\"]\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 5000\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"27Jul_0.0_0.4\"]*2\n",
    "prefixes = [\"lowsel\", \"highsel\"]\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 5000\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Now the overlay simulations, with genetic and nongenetic selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"1Aug_1.0_\" + suff for suff in [\"strength_1\", \"strength_2\", \"strength_5\", \"freq_10\", \"freq_100\", \"freq_1000\"]]\n",
    "used_filenames = [f+\"/\"+f for f in filenames]\n",
    "prefixes = [\"lowsel\"]*6\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 5005\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(used_filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"1Aug_0.98_\" + suff for suff in [\"strength_1\", \"strength_2\", \"strength_5\", \"freq_10\", \"freq_100\", \"freq_1000\"]]\n",
    "used_filenames = [f+\"/\"+f for f in filenames]\n",
    "prefixes = [\"lowsel\"]*6\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 5005\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(used_filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"1Aug_0.95_\" + suff for suff in [\"strength_1\", \"strength_2\", \"strength_5\", \"freq_10\", \"freq_100\", \"freq_1000\"]]\n",
    "used_filenames = [f+\"/\"+f for f in filenames]\n",
    "prefixes = [\"lowsel\"]*6\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 5005\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(used_filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"1Aug_0.9_\" + suff for suff in [\"strength_1\", \"strength_2\", \"strength_5\", \"freq_10\", \"freq_100\", \"freq_1000\"]]\n",
    "used_filenames = [f+\"/\"+f for f in filenames]\n",
    "prefixes = [\"lowsel\"]*6\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 5005\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(used_filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"1Aug_0.75_\" + suff for suff in [\"strength_1\", \"strength_2\", \"strength_5\", \"freq_10\", \"freq_100\", \"freq_1000\"]]\n",
    "#used_filenames = [f+\"/\"+f for f in filenames]\n",
    "prefixes = [\"lowsel\"]*6\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 5005\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename+\"/\"+filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        #print(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"1Aug_0.5_\" + suff for suff in [\"strength_1\", \"strength_2\", \"strength_5\", \"freq_10\", \"freq_100\", \"freq_1000\"]]\n",
    "#filenames = [f+\"/\"+f for f in filenames]\n",
    "prefixes = [\"lowsel\"]*6\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 5005\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename+\"/\"+filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"1Aug_0.0_\" + suff for suff in [\"strength_1\", \"strength_2\", \"strength_5\", \"freq_10\", \"freq_100\", \"freq_1000\"]]\n",
    "#filenames = [f+\"/\"+f for f in filenames]\n",
    "prefixes = [\"lowsel\"]*6\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 5005\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename+\"/\"+filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "Now look at size, time, deme and replacement-rate tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"22Jul_2m\", \"22Jul_10m\", \"22Jul_20m\"]*3\n",
    "used_filenames = [\"22Jul_tests_12Aug\"+\"/\"+f for f in filenames]\n",
    "prefixes = [\"nosel\"]*3 + [\"lowsel\"]*3 + [\"highsel\"]*3\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 205\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(used_filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"22Jul_6mo\", \"22Jul_18mo\", \"22Jul_2y\"]*3\n",
    "used_filenames = [\"22Jul_tests_12Aug\"+\"/\"+f for f in filenames]\n",
    "prefixes = [\"nosel\"]*3 + [\"lowsel\"]*3 + [\"highsel\"]*3\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 205\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(used_filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"22Jul_deme_1k\", \"22Jul_deme_5k\", \"22Jul_deme_20k\"]*3\n",
    "used_filenames = [\"22Jul_tests_12Aug\"+\"/\"+f for f in filenames]\n",
    "prefixes = [\"nosel\"]*3 + [\"lowsel\"]*3 + [\"highsel\"]*3\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 205\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(used_filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [\"22Jul_rep_0.25\", \"22Jul_rep_0.5\", \"22Jul_rep_0.75\"]*3\n",
    "used_filenames = [\"22Jul_tests_12Aug\"+\"/\"+f for f in filenames]\n",
    "prefixes = [\"nosel\"]*3 + [\"lowsel\"]*3 + [\"highsel\"]*3\n",
    "matrix_names = [filename+\"_\"+prefix for filename, prefix in zip(filenames, prefixes)]\n",
    "\n",
    "\n",
    "max_counts = 205\n",
    "\n",
    "for (filename, prefix, matrix_name) in zip(used_filenames, prefixes, matrix_names):\n",
    "    for no_samples in range(2, 9):\n",
    "        print(\"For \"+str(no_samples) + \" samples\", prefix)\n",
    "        stat_matrix = []\n",
    "        for number in range(1, max_counts+1):\n",
    "            try:\n",
    "                stats = calculate_output_stats(filename, prefix, base_filepath, number, no_samples)\n",
    "                stat_matrix.append(stats)\n",
    "                #print(stats)\n",
    "            except:\n",
    "                print(filename, prefix, matrix_name, number, \"failed\")\n",
    "        np.save(base_filepath+\"/\"+matrix_name+\"_\"+str(no_samples)+\"_samples.npy\", stat_matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "And now for the application to real tumours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in mutations and correct for clonality\n",
    "df = pd.read_csv(\"C:/Users/44749/Documents/PhD_work/datasets_for_analysis/full_tx421primary_muttable_vaf_ccf_clonality.csv\")\n",
    "\n",
    "print(\"Table loaded\")\n",
    "\n",
    "filtered_df= df[df['PASS'] == True & ((df['Timing_SC'] != \"E\") & (df['Timing_SC'] != \"N\"))]\n",
    "\n",
    "unique_tumours = filtered_df[['cruk_tumour_id']].drop_duplicates()\n",
    "\n",
    "purity_df = \n",
    "\n",
    "#translate table into dictionary\n",
    "\n",
    "driver_mut_dict = {True:1, False:0}\n",
    "all_tumour_names = []\n",
    "\n",
    "for tumour in unique_tumours.values.tolist():\n",
    "    tID = tumour[0]\n",
    "    all_drivs_found = []\n",
    "    all_tumour_names.append(tID)\n",
    "    df_here = filtered_df[filtered_df['cruk_tumour_id'] == tID]\n",
    "    unique_regions = df_here[['region']].drop_duplicates()\n",
    "    #print(unique_regions)\n",
    "    tumour_regions = [{} for m in range(len(unique_regions.values.tolist()))]\n",
    "    #print(unique_regions)\n",
    "    for n, region in enumerate(unique_regions.values.tolist()):\n",
    "        regID = region[0]\n",
    "        #print(tID, regID)\n",
    "        relevant_muts = df_here[df_here['region']==regID]\n",
    "        #print(len(relevant_muts))\n",
    "        for index, row in relevant_muts.iterrows():\n",
    "            mutID = str(row['chr']) + \".\" +str(row['start']) + \".\" + str(row['ref']) + \".\" + str(row['var']) + \":\" + str(row['Hugo_Symbol']) + \":\" + str(driver_mut_dict[row['DriverMut']])\n",
    "            ccf = min(1, float(row['final_ccf'])) if row['cluster_clonality'] != 'clonal' else 1\n",
    "            #print(tID, regID, mutID, ccf)\n",
    "            #print(regID, ccf)\n",
    "            if ccf > 0:\n",
    "                tumour_regions[n][mutID] = ccf\n",
    "                if row['DriverMut']:\n",
    "                    all_drivs_found.append(row['Hugo_Symbol']) #add driver to list if it's found here, might be added more than once- record only the Hugo Symbol, NOT the specific mutation\n",
    "                    #print(mutID)\n",
    "        # print(tumour_regions[n])\n",
    "    np.save(tID+\"_mutdict.npy\", tumour_regions)\n",
    "    \n",
    "    np.save(base_filepath+\"/\"+tID+\"_driver_muts.npy\", np.unique(all_drivs_found))\n",
    "\n",
    "np.save(\"all_tumour_names.npy\", all_tumour_names)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that will do this for individual tumours\n",
    "\n",
    "#calculate output summary statistics for a given number of samples, excluding clonal mutations\n",
    "def calculate_output_stats_real_tumour(tumour_name):\n",
    "    mutdict = np.load(tumour_name + \"_mutdict.npy\", allow_pickle=True)\n",
    "    no_samples = len(mutdict)\n",
    "    all_muts = []\n",
    "    for dict in mutdict:\n",
    "        all_muts += list(dict.keys()) #look at all mutations seen in this sample\n",
    "    all_unique_mutations = np.unique(all_muts) #all mutations in all samples\n",
    "    mut_matrix = np.zeros((len(all_unique_mutations), no_samples))\n",
    "    for m, mut in enumerate(all_unique_mutations):\n",
    "        for n, dict in enumerate(mutdict):\n",
    "            if mut in dict:\n",
    "                mut_matrix[m][n] = dict[mut] #zero by default\n",
    "    #now proceed as with a simulated tumour, excluding clonal samples\n",
    "    min_ccfs = np.min(mut_matrix, axis=1)\n",
    "    non_clonal_muts = np.where(min_ccfs < 1.0)[0] #not clonal everywhere\n",
    "    mut_matrix = mut_matrix[non_clonal_muts] #filter out all of the clonal ones\n",
    "    #now to actually calculate summary statistics!\n",
    "    #0. all mutations detected subclonally (i.e. with CCF < 1.0 in at least one sample)\n",
    "    no_muts = len(non_clonal_muts)\n",
    "    #1. all mutations detected in 1 sample only\n",
    "    appearances = np.count_nonzero(mut_matrix, axis=1)\n",
    "    private = np.where(appearances == 1)[0]\n",
    "    num_private = len(private)\n",
    "    #2-no_samples, ... number fixed in 1, ... no_samples-1 samples\n",
    "    fixed_muts = []\n",
    "    for s in range(1, no_samples):\n",
    "        fixed_muts.append(count_fixation_number(mut_matrix, s)) #count number fixed in s samples\n",
    "    #no_samples+1: number of mutations fixed AND private\n",
    "    private_ccfs = np.max(mut_matrix[private], axis=1) #the maximum CCF of a private mutation will be its CCF in that sample\n",
    "    fixed_and_private = np.where(private_ccfs == 1.0)[0]\n",
    "    num_fixed_private = len(fixed_and_private)\n",
    "    #no_samples + 2-2*no_samples-1- the number of mutations appearing in 2, ... no_samples samples\n",
    "    num_appearing = []\n",
    "    for s in range(2, no_samples+1):\n",
    "        num_appearing.append(len(np.where(appearances == s)[0]))\n",
    "    return no_samples, [no_muts, num_private] + fixed_muts + [num_fixed_private] + num_appearing    \n",
    "    \n",
    "\n",
    "tIDs = np.load(\"all_tumour_names.npy\", allow_pickle=True)\n",
    "\n",
    "print(len(tIDs))\n",
    "\n",
    "#assign tumours to a different bucket depending on the number of samples taken\n",
    "stats_by_sample_number = [[] for s in range(2, 9)]\n",
    "num_samples = {}\n",
    "for tumour_name in tIDs:\n",
    "    try:\n",
    "        no_samples, stats = calculate_output_stats_real_tumour(tumour_name)\n",
    "        if no_samples > 1:\n",
    "            num_samples[tumour_name] = no_samples\n",
    "            stats_by_sample_number[no_samples-2].append(stats)\n",
    "            np.save(base_filepath+\"/\"+tumour_name+\"_ss_4424.npy\", stats)\n",
    "            print(tumour_name, \"found\")\n",
    "        else:\n",
    "            print(tumour_name, \"too few samples\")\n",
    "            \n",
    "    except:\n",
    "        print(tumour_name, \"not found\")\n",
    "\n",
    "#print(overall_stats)\n",
    "\n",
    "for n, stat_matrix in enumerate(stats_by_sample_number):\n",
    "    np.save(\"cruk_\"+str(n+2)+\"_samples_stat.npy\", stat_matrix)\n",
    "\n",
    "np.save(\"num_samples_by_tumour_name.npy\", num_samples)\n",
    "#np.save(\"stat_index_by_tumour_name.npy\", stat_index_dict)\n",
    "#np.save(\"cruk_overall_stats.npy\", overall_stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Get purity of each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x):\n",
    "    x += 1\n",
    "    return x\n",
    "\n",
    "x=1\n",
    "for i in range(10):\n",
    "    test(x)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
